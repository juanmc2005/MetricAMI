# A Metric Learning Approach to Misogyny Categorization

This is the companion repository for the paper "A Metric Learning Approach to Misogyny Categorization" at the 5th Workshop on Representation Learning for NLP, ACL 2020.

The project contains all the code needed to run the experiments.

## Installation

We recommend you first install PyTorch as described in the [official website](https://pytorch.org/) according to your hardware capabilities, making sure the version is at least 1.4.0 (ideally you should install exactly this version to avoid any compatibility issues).

1. Clone the repository and `cd MetricAMI`
2. Run `pip install -r requirements.txt`

## Prepare the Dataset

1. Obtain the official Evalita 2018 AMI dataset from [here](https://github.com/evalita2018/data/tree/master/AMI)
2. Run the preparation script to pre-tokenize the data and split the training set into train and dev:

```shell
python prepare.py --path <AMI_PATH> --out <OUTPUT_PATH>
```

where `<AMI_PATH>` is the directory containing `en_training.tsv` and `en_testing.tsv`, and `<OUTPUT_PATH>` any directory to put the preprocessed data with `en` as the last directory. This latter requirement specifies the language of the corpus.

Notice that the vocabulary will also be generated by this script.

At this point you should have a directory tree like this:
```
- <NEW_AMI_PATH>/
  - en/
    - train/
    - dev/
    - test/
    - ami_vocab.txt
```

## Train an LSTM Model

To train this model you will need the generated vocabulary as well as a word2vec GENSIM model (soon to be uploaded).
You can of course train any of the other loss functions in a similar manner by modifying the parameters (see [here](args.py) for the parameter list and description).

```shell
python train.py \
       --path <NEW_AMI_PATH> \
       --vocab /path/to/ami_vocab.txt \
       --word2vec-model word_embeddings/Cbowwiki_en_data_iter5_vec_size_300 \
       --epochs 60 --save --batch-size 32 --log-interval 50 \
       --model lstm --loss softmax --lr 1e-3 \
       --exp-path <EXPERIMENT_PATH>
```

## Train a BERT Model

Training BERT is easier thanks to the Huggingface library, you just need to run the following script.
As with the LSTM, any other loss function from the paper can also be trained (see [here](args.py)).

```shell
python train.py \
       --path <NEW_AMI_PATH> \
       --epochs 60 --save --batch-size 32 --log-interval 50 \
       --model bert --loss softmax --lr 1e-5 \
       --exp-path <EXPERIMENT_PATH>
```

## Citation

If our work has been useful to you, please cite our paper:

```bibtex
To be added soon!
```
